# Dublin Region Energy Masterplan Code Guide

## Directory structure

The directory structure of the dublin_energy_masterplan was copied from the [cookiecutter-data-science](https://github.com/drivendata/cookiecutter-data-science) repository, here's their explanation:

```
├── LICENSE
├── Makefile           <- Makefile with commands like `make data`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── docs               <- A default Sphinx project; see sphinx-doc.org for details
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
|
├── tests              <- Test scripts made using the pytest library 
│
├── src                <- Source code for use in this project.
│   ├── __init__.py    <- Makes src a Python module
│   │
│   ├── data           <- Scripts to download or generate data
│   │   └── make_dataset.py
│   │
│   ├── preprocess     <- Scripts to clean the data 
│   │
│   ├── features       <- Scripts to turn raw data into features for modeling
│   │   └── build_features.py
│   │
│   ├── models         <- Scripts to train models and then use trained models to make
│   │   │                 predictions
│   │   ├── predict_model.py
│   │   └── train_model.py
│   │
│   └── visualization  <- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py
│
└── tox.ini            <- tox file with settings for running tox; see tox.testrun.org
```

---

## Glossary of terms

- __notebook__ = a Jupyter notebook (.ipynb) file that can be run cell-by-cell - so plots, spreadsheets and data generated by code in the cells can be viewed instantly below it. 

- __script__ = a text file containing functions and classes - Python scripts have .py file endings.  These files must be run via the command line (See section below for more details).

- __module__ = an individual python file containing a bunch of functions or classes.  

- __library__ = a collection of modules with helpful functions and classes.

---

## Setup a Local Development environment using Visual Studio Code

1. Download the [dublin_energy_masterplan folder](https://github.com/RowanMolony/dublin_energy_masterplan) from this Github repository by clicking 'Clone or download' to your Downloads folder and unzip it.  Make sure that the folder containing the contents of this repository is named 'dublin_energy_masterplan'
   
2. Download [Microsoft Visual Studio Code](https://code.visualstudio.com/) and __click open with code option on install__ during installation
<br>

3. Install the [Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)
<br>

4. Launch Visual Studio Code in the _dublin_energy_masterplan_ folder and click Jupyter Notebooks or Python scripts to view, run and edit

    - __Option 1:__ With the _open with Code_ option enabled it is possible to right-click the _dublin_energy_masterplan_ folder and open it in Visual Studio code.
    - __Option 2:__ Open _Visual Studio code_ and open the _dublin_energy_masterplan_ folder using the File option on the ribbon toolbar.
  <br>

4. Download the entire [_data_ folder](https://drive.google.com/drive/u/0/folders/1HIS60VwXROUHVqwxGjG9FIzk8rNPIZNM) from the Shared Drive, extract all compressed files within each subfolder and move the data folder to the same folder as the _dublin_energy_masterplan_.  Make sure that the folder containing the data is named 'data'
<br>

5.  Install and setup a Conda Virtual environment: 

    - Download and install either [miniconda](https://docs.conda.io/en/latest/miniconda.html) or [anaconda](https://www.anaconda.com/distribution/)

    - Recreate the masterplan virual environment from the file _environment.yml_ by opening Anaconda Prompt and entering:

      ```Powershell
      PS C:\Users\RowanM> cd Downloads
      PS C:\Users\RowanM\Downloads> cd dublin_energy_masterplan
      PS C:\Users\RowanM\Downloads\dublin_energy_masterplan> conda env create -f environment.yml
      ```
    
      -> For more information on using the command line see section [how to run scripts](#how-to-run-scripts) (99% of the time all you need is two commands: `cd` and `ls`)

6. Re-open VSCode and set the default Python interpeter to the 'masterplan':conda environment by selecting View > Command Palette from the toolbar (or CTRL+Shift+P) and entering:  

    ```
    Python: Select Interpreter
    ```

7. You should now be able to open any file via the _EXPLORER_ outline:

    - The notebooks are the most straightforward to run as code can be executed cell by cell by clicking the play button (or Shift-Enter) -> See section on [notebooks](#jupyter) 
  
    - The Python scripts can be run in either [iPython](https://ipython.readthedocs.io/en/stable/) or directly from the command line using `python <name-of-script>.py` -> See section on [how to run scripts](#how-to-run-scripts) 
<br>

---

## [Optional] Setup Windows Subsystem for Linux (WSL)

A lot of Python modules require tedious workarounds to run on Windows so currently I'm running all of my code in WSL.  If you're having install issues with the previous setup it may be worthwhile switching to WSL.

1. Download & install Windows Subsystem for Linux (WSL) - I've linked in the [VS Code WSL extension guide](https://code.visualstudio.com/docs/remote/wsl).


2. Set default terminal to WSL by pressing `CONTROL-SHIFT-P`, type Terminal: Select Default Shell and select WSL Bash.

3. Download [Miniconda3 for Linux](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html) via the WSL terminal in VS Code:

      ```bash
      root@Rowan 
      root@Rowan $ cd /mnt/c/Users/RowanM
      root@Rowan /mnt/c/Users/RowanM $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh 
      root@Rowan /mnt/c/Users/RowanM $ bash Miniconda-latest-Linux-x86_64.sh
      ```

    - Accept all terms when prompted by Terminal 
<br>

4. Launch dublin_energy_masterplan in WSL. 

    - __Option 1:__ Click the green button at the bottom right-hand side of your screen and select WSL: Ubuntu
    - __Option 2:__ Launch VS Code in the dublin_energy_masterplan folder using the bash terminal:

      ```bash
      (masterplan) root@Rowan /mnt/c/Users/RowanM/Documents/dublin_energy_masterplan (master) $ code .
      ```

5. Install all required libraries via in the masterplan conda environment:

    ```bash
      (base)root@Rowan:/mnt/c/Users/RowanM/Documents/dublin_energy_masterplan# conda env create --file=environment.yml
      (base)root@Rowan:/mnt/c/Users/RowanM/Documents/dublin_energy_masterplan# source activate masterplan 
      (masterplan)root@Rowan:/mnt/c/Users/RowanM/Documents/dublin_energy_masterplan# ...
    ```

---

## Installing more packages

__With conda__:

- Using Terminal & Conda/pip (via WSL and miniconda) - i.e. can either run `conda install <package>` or `pip install package` if conda doesn't have it
<br>

- Using Terminal & Conda/pip via Docker (enables running of Unix-based libraries on Windows computers).

The Conda approach is more straightforward and is fine for running all of the code used so far here.  The Docker approach may be more useful in the future if libraries such as Luigi or Snakemake or PyPSA are used.

__Without Conda (try if conda doesn't have package):__

1. Using Terminal & pip only:

    - Install recommended packages stored in requirements.txt
    ``` 
    pip install -r requirements.txt 
    ```
    
    - Install missing packages: 
    ```
    pip install pandas
    ```  

    __Note:__ may need to `pip install -e .` in the _dublin_energy_masterplan_ folder to install the local _src_ library (which contains modules to load, save & preprocess data)

---

## Package not found?

You will probably see this prompt frequently:

```Python
ModuleNotFoundError: No module named 'dpcontracts'
```

Now that Conda has been setup solving this issue just involves using `pip` or `conda` (in this case pip):

```python
pip install dpcontracts
```

---

## Jupyter

For an introduction to jupyter notebooks skip to 04:00 @ https://www.youtube.com/watch?v=HW29067qVWk

The VS Code notebook can be run in a similar way with inline execution.

Notebooks (.ipnb files) are handy for quickly running code in cells and seeing what the code does by plotting outputs in spreadsheet form , in graphical form or even spatially.

__Example:__

Load data in and show it:

![Showing data spreadsheets](images/show_dataframe_head.PNG)

Load in data and plot it:

![Showing plots](images/mapping_Dublin.PNG)

- Loading in data is more straightforward than the above examples when the data is stored in the same folder as the notebook or script:
```python
data = pd.read_excel('some_file.xlsx')
```

- Showing the top of the spreadsheet:
```python
data.head()
```

- Plotting the data:
```Python
data.plot()
```

__Note:__ The full notebook of reading in data and plotting it on a map of Dublin can be seen by opening example_notebook.html

---

## Using existing libraries

```Python
import pandas  
```

Now all of the pandas functions and classes can be accessed with a `.` operator:

```Python
# Create a DataFrame containing the list [1 2 3]
data = pandas.DataFrame([1 2 3])
```

Typically use `import pandas as pd` instead as short-hand so:

```Python
data = pd.DataFrame([1 2 3])
```


---

## Why use scripts at all?

Notebooks are great for prototyping ideas with single-use code but not so good for writing reusable code.  Scripts are preferable for this purpose as they make the following possible:

- __Refactoring__ = re-implementing code to speed it up or clean it up.
- __Testing__ = write test scripts to make sure the code does what it says on the tin (trademark: Ronsill??)
- __Logging__ = store outputs of code as it runs in external files so can see what's going on inside the file
- __data pipeline__ = specifies the process of data transformation (where it comes from, where it goes, what's done to it) - typically a helper module such as [Luigi](https://luigi.readthedocs.io/en/stable/) is used for this.  
 
---

## How to run scripts?

As scripts are run using the command line it is necessary to be familiar with a few commands (surprisingly few are needed)

- `cd` = change directory
- `cd ..` = go back a directory
- `ls` = list names of files and folders in directory
- `pwd` = print current directory
- `cd <name-of-file-or-folder>` = go to file/folder

See [Command Line Crash Course](https://learnpythonthehardway.org/book/appendixa.html) from Learn Python The Hard Way (the entire pdf is on the Share Drive) for intros/examples of maybe 10 commands that are used all the time.

__Note__: By typing `cd D<TAB>` you can Tab through available files starting with D and press ENTER once happy with the selection. To go back to the parent directory use the special symbol `..` which represents _the parent of this directory - so `cd ..` means go to the parent of this directory.

I currently launch iPython in my Conda environment and run scripts using `%run <name-of-script>.py` to execute scripts.  It is possible to run iPython without creating a Conda envionment but installing existing Python libraries (& non-Python libraries) is not as straightforward.


---


## Why Conda?

Conda allows for easy installation of any required Python libraries by using the commands `conda` or `pip`:

```Powershell
(base) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan> conda install pandas
```
Can also easily install multiple libraries at once:

```Powershell
(base) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan> pip install numpy geopandas
```

Conda makes installations much easier as it acts as a virtual environment to which you can install all of your packages to avoid conflicts.  If you want to you can create multiple environments each with different versions of Python and Python libraries.  You can also install non-Python content such as R and SQL.

---

## [iPython](https://ipython.readthedocs.io/en/stable/)

The command line tool iPython can be used to run scripts and try out code interactively in the same way as Notebook:

```Powershell
(masterplan) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan> cd src/data  
(masterplan) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan\src\data> ipython
Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.11.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]:   
```

iPython is an interactive tool currently being used to run individual modules.  Running files involves running iPython in the same directory as the Python scripts and using the `run` magic command:

```Powershell
(masterplan) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan> cd src/data  
(masterplan) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan\src\data> ipython
Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 7.11.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: ls
Volume in drive C is OS
Volume Serial Number is 1667-F640

Directory of C:\Users\RowanM\Documents\dublin_energy_masterplan\src\data

23/03/2020  14:01    <DIR>          .
23/03/2020  14:01    <DIR>          ..
05/03/2020  11:29    <DIR>          .empty
23/03/2020  09:13    <DIR>          .ipynb_checkpoints
23/03/2020  11:04    <DIR>          .mypy_cache
23/03/2020  11:04                 0 __init__.py
23/03/2020  17:08    <DIR>          __pycache__
23/03/2020  11:04             1,130 BER_columns.py
23/03/2020  11:04            66,413 df_redistributed.csv
23/03/2020  17:08             1,598 load.py
23/03/2020  17:05             3,209 path.py
23/03/2020  11:04            14,787 preprocess_BER.py
23/03/2020  11:04             5,218 preprocess_Census2016.py
23/03/2020  11:04             1,075 save.py
              8 File(s)         93,430 bytes
              6 Dir(s)  58,190,192,640 bytes free
In [2]: run preprocess_BER.py  
In [3]: import pandas as pd
In [4]: data = pd.DataFrame([1 2 3])
```

__Note:__ There's a whole chapter on iPython at [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) if you want more details.

---

## Why Docker?

Some Python libraries don't work in Windows or require tedious workarounds.  Docker lets us create a Unix container that acts like a Virtual Machine but with much less overhead as it doesn't come with the baggage of a Graphics User Interface. Docker containers enable encapsulation of the entire development environment so that code will run in exactly the same way on any computer.

Major Python packages such as Pandas and Numpy run fine on Conda alone.

---

## Setting up a Docker development environment

1. Install [docker](https://docs.docker.com/docker-for-windows/)

2. Navigate to the _docker_ directory in _dublin_energy_masterplan_ and build the masterplan docker image

    ```Powershell
    (base) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan> cd docker
    (base) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan\docker> docker build -t mplan:0.0.1 .
    ```

3. Create a docker container using the masterplan docker images and launch bash (Unix's equivalent to Windows Command Prompt or Powershell)

    ```Powershell
    (base) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan\docker> cd ..
    (base) PS C:\Users\RowanM\Downloads\dublin_energy_masterplan>  docker run -it `
      -v ${PWD}:/home `
      -w /home/ `
      mplan:0.3 `
      bash
    ```


## How to use the Docker container

Once the container is running can execute scripts using iPython as shown in above example.

---

    Project name: Dublin Region Energy Masterplan
    Today's date: 20/03/2020
    Maintainer's contact info: rowan.molony@codema.ie
    Data Origin: <TO DO>

    3-4 sentences about the goal of the project
    Dependencies / How to Install / Project Structure / Style Guide
